{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\felip\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../../Data/train/df_us_train.pickle\"\n",
    "df_us_train = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "path =  \"../../Data/train/df_es_train.pickle\"\n",
    "df_es_train = pickle.load(open(path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "\n",
    "stopwords_spanish = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
    ").values\n",
    "stopwords_spanish = Counter(stopwords_spanish.flatten().tolist())\n",
    "\n",
    "stopwords_english = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords_english = Counter(stopwords_english.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contructor de sentence-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 12:35:01,722 : INFO : loading Word2Vec object from us_emoji_w2v.model\n",
      "2022-06-30 12:35:01,733 : INFO : loading wv recursively from us_emoji_w2v.model.wv.* with mmap=None\n",
      "2022-06-30 12:35:01,734 : INFO : setting ignored attribute cum_table to None\n",
      "2022-06-30 12:35:01,758 : INFO : Word2Vec lifecycle event {'fname': 'us_emoji_w2v.model', 'datetime': '2022-06-30T12:35:01.758834', 'gensim': '4.2.0', 'python': '3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "us_emoji_w2v = Word2Vec.load(\"us_emoji_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 12:35:03,378 : INFO : loading Word2Vec object from es_emoji_w2v.model\n",
      "2022-06-30 12:35:03,390 : INFO : loading wv recursively from es_emoji_w2v.model.wv.* with mmap=None\n",
      "2022-06-30 12:35:03,391 : INFO : setting ignored attribute cum_table to None\n",
      "2022-06-30 12:35:03,395 : INFO : Word2Vec lifecycle event {'fname': 'es_emoji_w2v.model', 'datetime': '2022-06-30T12:35:03.395021', 'gensim': '4.2.0', 'python': '3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es_emoji_w2v = Word2Vec.load(\"es_emoji_w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc, stopwords, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans(\n",
    "            '', '', punctuation)).lower().split()\n",
    "\n",
    "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
    "\n",
    "    tokenized_doc = [\n",
    "        token for token in tokenized_doc if token.lower() not in stopwords\n",
    "    ]\n",
    "    return tokenized_doc\n",
    "\n",
    "def sentence_embedding_from_w2v(doc, w2v):\n",
    "    pre_tokens = simple_tokenizer(doc, stopwords_spanish, lower=False)\n",
    "    sentence_vect = np.zeros(w2v.vector_size)\n",
    "    m = 0\n",
    "    for token in pre_tokens:\n",
    "        if token in w2v.wv:\n",
    "            sentence_vect += w2v.wv[token]\n",
    "            m += 1\n",
    "    sentence_vect *= 1/m if m>0 else 1\n",
    "    return sentence_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "us_train_w2v_sentence_embedding = pd.DataFrame(np.array(\n",
    "    [ sentence_embedding_from_w2v(doc, us_emoji_w2v) for doc in df_us_train[\"text\"]]\n",
    "))\n",
    "us_train_w2v_sentence_embedding[\"id\"] = df_us_train[\"id\"]\n",
    "us_train_w2v_sentence_embedding[\"label\"] = df_us_train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es_train_w2v_sentence_embedding = pd.DataFrame(np.array(\n",
    "    [ sentence_embedding_from_w2v(doc, es_emoji_w2v) for doc in df_es_train[\"text\"]]\n",
    "))\n",
    "es_train_w2v_sentence_embedding[\"id\"] = df_es_train[\"id\"]\n",
    "es_train_w2v_sentence_embedding[\"label\"] = df_es_train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 383 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_us = us_train_w2v_sentence_embedding.drop(columns=[\"label\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 91.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_es = es_train_w2v_sentence_embedding.drop(columns=[\"label\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_labels_us = us_train_w2v_sentence_embedding[\"label\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_labels_es = es_train_w2v_sentence_embedding[\"label\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.metrics.cluster import rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
