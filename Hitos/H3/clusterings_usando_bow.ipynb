{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\felip\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\felip\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "from time import time  \n",
    "from collections import defaultdict \n",
    "import string \n",
    "import multiprocessing\n",
    "import os\n",
    "import gensim\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../../Data/train/df_us_train.pickle\"\n",
    "df_us_train = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "path =  \"../../Data/train/df_es_train.pickle\"\n",
    "df_es_train = pickle.load(open(path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
    "\n",
    "stopwords_spanish = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
    ").values\n",
    "stopwords_spanish = Counter(stopwords_spanish.flatten().tolist())\n",
    "\n",
    "stopwords_english = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
    ").values\n",
    "stopwords_english = Counter(stopwords_english.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contructor de sentence-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmerTokenizer:\n",
    "    def __init__(self, stopwords):\n",
    "        self.ps = PorterStemmer()\n",
    "        self.sw = stopwords\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        doc_tok = word_tokenize(doc)\n",
    "        doc_tok = [t for t in doc_tok if t not in self.sw]\n",
    "        return [self.ps.stem(t) for t in doc_tok]\n",
    "\n",
    "# Inicializamos tokenizador\n",
    "tokenizador_english = StemmerTokenizer(stopwords_english)\n",
    "tokenizador_spanish = StemmerTokenizer(stopwords_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_english = CountVectorizer(\n",
    "    tokenizer= tokenizador_english,\n",
    "    ngram_range=(1,1),\n",
    ")\n",
    "\n",
    "bow_spanish= CountVectorizer(\n",
    "    tokenizer= tokenizador_spanish,\n",
    "    ngram_range=(1,1),\n",
    ")\n",
    "\n",
    "ct_bow_english = ColumnTransformer([\n",
    "    (\"BOW\", bow_english, \"text\")\n",
    "])\n",
    "\n",
    "ct_bow_spanish = ColumnTransformer([\n",
    "    (\"BOW\", bow_spanish, \"text\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_bow_english = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing', ct_bow_english)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_bow_spanish = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessing', ct_bow_spanish)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
